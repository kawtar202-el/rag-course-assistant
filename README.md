# RAG Course Assistant

A Retrieval-Augmented Generation (RAG) application to help students ask questions about their course materials using open-source language models from Hugging Face.

## Features

- Process course materials from the `course_materials` folder
- Multiple interface options: command-line, web chat, and Streamlit
- Get answers generated by open-source LLMs
- Clean chat interface with message bubbles

## Prerequisites

- Python 3.8+
- Hugging Face API Token (get one at [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens))

## Installation

1. Clone this repository
2. Install requirements:
   ```
   pip install -r requirements.txt
   ```
3. Add your Hugging Face API token to `.env`:
   ```
   HUGGINGFACEHUB_API_TOKEN=your_token_here
   ```

## Usage

### Web Chat Interface (New!)

1. Place your course materials in the `course_materials` folder
2. Start the API server:
   ```
   python -m uvicorn api:app --host 0.0.0.0 --port 8000
   ```
   Or use the batch file:
   ```
   start_api.bat
   ```
3. Open `chat_interface.html` in your web browser
4. Ask questions about your course materials

### Command-Line Interface (Recommended)

1. Place your course materials in the `course_materials` folder
2. Run the command-line interface:
   ```
   python cli_interface.py
   ```
   Or use the simple version:
   ```
   python cli_simple.py
   ```
   Or use the batch files:
   ```
   start_cli.bat
   start_simple_cli.bat
   ```
3. Ask questions about your course materials directly in the terminal

### Streamlit Web Interface (Alternative)

1. Place your course materials in the `course_materials` folder
2. Run the Streamlit app:
   ```
   streamlit run app.py
   ```
3. The application will be available at `http://localhost:8501`

## How It Works

1. Documents are loaded and split into chunks
2. Embeddings are created using sentence transformers
3. Vector store is built using FAISS for efficient similarity search
4. When you ask a question, relevant document chunks are retrieved
5. An open-source LLM generates an answer based on the retrieved context

## Technologies Used

- LangChain for orchestration
- Hugging Face models for LLM and embeddings
- FAISS for vector storage
- FastAPI for web backend
- HTML/CSS/JavaScript for web frontend
- Streamlit for the web interface (optional)
- PyPDF2 for PDF processing